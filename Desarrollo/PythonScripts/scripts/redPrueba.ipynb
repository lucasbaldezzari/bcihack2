{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (Conv2D, BatchNormalization, ELU, AveragePooling2D,\n",
    "                          Dropout, Permute, Reshape, GRU, Dense, DepthwiseConv2D,\n",
    "                          Activation, SeparableConv2D)\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN_EEGNet:\n",
    "    def __init__(self, input_shape, num_classes, F1=8, D=2, dropout_rate=0.4, l2_lambda=0.001):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        C, T = self.input_shape\n",
    "        model = Sequential()\n",
    "\n",
    "        # Bloque 1: Capas convolucionales\n",
    "        model.add(Reshape((C, T, 1), input_shape=self.input_shape))\n",
    "        model.add(Conv2D(self.F1, (C, 1), padding='same', activation='linear', kernel_regularizer=l2(self.l2_lambda)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(DepthwiseConv2D((1, T), depth_multiplier=self.D, depthwise_constraint='max_norm', activation='linear', depthwise_regularizer=l2(self.l2_lambda)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(AveragePooling2D((4, 1)))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "\n",
    "        # Capas recurrentes\n",
    "        model.add(Permute((1, 3, 2)))\n",
    "        model.add(Reshape((-1, self.F1 * self.D)))\n",
    "        model.add(GRU(64, dropout=self.dropout_rate))\n",
    "\n",
    "        # Capa clasificadora\n",
    "        model.add(Dense(self.num_classes, activation='softmax', kernel_regularizer=l2(self.l2_lambda)))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile(self, optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):\n",
    "        \"\"\"\n",
    "        Método para compilar el modelo.\n",
    "        ...\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit(self, x_train, y_train, batch_size=32, epochs=10, validation_data=None,\n",
    "        patience=10, model_checkpoint_path=\"best_model.h5\"):\n",
    "        \"\"\"\n",
    "        Método para entrenar el modelo CRNN_EEGNet.\n",
    "\n",
    "        Parámetros:\n",
    "            - x_train (array): Datos de entrenamiento.\n",
    "            - y_train (array): Etiquetas de entrenamiento.\n",
    "            - batch_size (int, opcional): Tamaño del lote para el entrenamiento. Por defecto es 32.\n",
    "            - epochs (int, opcional): Número máximo de épocas para entrenar el modelo. Por defecto es 10.\n",
    "            - validation_data (tuple, opcional): Datos de validación en el formato (x_val, y_val).\n",
    "                                                Si se proporciona, se usa para validar el modelo después de cada época.\n",
    "            - patience (int, opcional): Número de épocas sin mejora en la pérdida de validación para activar\n",
    "                                        el early stopping. Por defecto es 10.\n",
    "            - model_checkpoint_path (str, opcional): Ruta donde se guardará el mejor modelo basado en la pérdida\n",
    "                                                    de validación. Por defecto es \"best_model.h5\".\n",
    "\n",
    "        Devuelve:\n",
    "            - history (History): Objeto con los registros del entrenamiento, que incluye la pérdida y las métricas\n",
    "                                para cada época.\n",
    "\n",
    "        Nota:\n",
    "            Este método utiliza early stopping para prevenir el sobreajuste. Si la pérdida de validación no mejora\n",
    "            durante un número de épocas especificado en 'patience', se detendrá el entrenamiento y se restaurarán\n",
    "            los pesos del modelo al mejor estado encontrado. Además, se guardará el mejor modelo durante el\n",
    "            entrenamiento en el archivo especificado en 'model_checkpoint_path'.\n",
    "        \"\"\"\n",
    "\n",
    "      # Verificar que se proporciona validation_data\n",
    "        if validation_data is None:\n",
    "            raise ValueError(\"validation_data must be provided when monitoring val_loss\")\n",
    "\n",
    "        # Definir callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(model_checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        self.history = self.model.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=[early_stopping, model_checkpoint]\n",
    "        )\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Método para evaluar el modelo en datos de prueba.\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Método para imprimir un resumen de la arquitectura del modelo.\n",
    "        \"\"\"\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (20, 500)\n",
    "num_classes = 5\n",
    "\n",
    "crnn_eegnet = CRNN_EEGNet(input_shape, num_classes)\n",
    "crnn_eegnet.compile()\n",
    "crnn_eegnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('C:/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/data.npy')\n",
    "y = np.load('C:/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "clase_a_eliminar = []\n",
    "\n",
    "indices_a_mantener = ~np.isin(y, clase_a_eliminar)\n",
    "X_filtrado = x[indices_a_mantener]\n",
    "y_filtrado = y[indices_a_mantener]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_filtrado = to_categorical(y_filtrado-1, num_classes= 5)\n",
    "\n",
    "eeg_train, eeg_test, labels_train, labels_test = train_test_split(X_filtrado, y_filtrado, test_size=0.1, stratify=y_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness = crnn_eegnet.fit(eeg_train, labels_train, epochs = 5000, validation_data = (eeg_test, labels_test), patience = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalaucion = crnn_eegnet.evaluate(eeg_test, labels_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
