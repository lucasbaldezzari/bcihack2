{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YHBYXo0b-OH8",
      "metadata": {
        "id": "YHBYXo0b-OH8"
      },
      "source": [
        "# Instalación librería"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mU1qKwJk-TzM",
      "metadata": {
        "id": "mU1qKwJk-TzM"
      },
      "source": [
        "# Carga librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "skQH8zOtvX4t",
      "metadata": {
        "id": "skQH8zOtvX4t"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from SignalProcessor.Filter import Filter\n",
        "from SignalProcessor.CSPMulticlass import CSPMulticlass\n",
        "from SignalProcessor.FeatureExtractor import FeatureExtractor\n",
        "from SignalProcessor.RavelTransformer import RavelTransformer\n",
        "\n",
        "\n",
        "## Clasificadores LDA y SVM\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
        "import pickle\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from TrialsHandler.TrialsHandler import TrialsHandler\n",
        "from TrialsHandler.Concatenate import Concatenate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rnWVMtAI-Wnz",
      "metadata": {
        "id": "rnWVMtAI-Wnz"
      },
      "source": [
        "# Subida de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "hoob1OpAdekW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoob1OpAdekW",
        "outputId": "5d5740fc-7a53-4ea2-900c-e63b4b5c2321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se han extraido 75 trials\n",
            "Se han extraido 8 canales\n",
            "Se han extraido 1000 muestras por trial\n",
            "Se han extraido 75 trials\n",
            "Se han extraido 8 canales\n",
            "Se han extraido 1000 muestras por trial\n",
            "(150, 6, 1000)\n",
            "(150,)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "### ********** Cargamos los datos **********\n",
        "file = \"data/sujeto_11/eegdata/sesion1/sn1_ts0_ct0_r1.npy\"\n",
        "eventosFile = \"data/sujeto_11/eegdata/sesion1/sn1_ts0_ct0_r1_events.txt\"\n",
        "rawEEG_1 = np.load(file)\n",
        "eventos_1 = pd.read_csv(eventosFile, sep = \",\")\n",
        "\n",
        "file = \"data/sujeto_11/eegdata/sesion2/sn2_ts0_ct0_r1.npy\"\n",
        "eventosFile = \"data/sujeto_11/eegdata/sesion2/sn2_ts0_ct0_r1_events.txt\"\n",
        "rawEEG_2 = np.load(file)\n",
        "eventos_2 = pd.read_csv(eventosFile, sep = \",\")\n",
        "\n",
        "#Creamos objetos para manejar los trials\n",
        "th_1 = TrialsHandler(rawEEG_1, eventos_1, tinit = 0, tmax = 4, reject=None, sample_rate=250.)\n",
        "th_2 = TrialsHandler(rawEEG_2, eventos_2, tinit = 0, tmax = 4, reject=None, sample_rate=250.)\n",
        "\n",
        "dataConcatenada = Concatenate([th_1, th_2])#concatenamos datos\n",
        "\n",
        "channelsSelected = [0,1,2,3,6,7]\n",
        "\n",
        "#me quedo con channelsSelected\n",
        "dataConcatenada.trials = dataConcatenada.trials[:,channelsSelected,:]\n",
        "\n",
        "# Estas son las clases que quieres mantener\n",
        "        # \"Mano Izquierda\",1\n",
        "        # \"Mano Derecha\",2\n",
        "        # \"Ambas Manos\",3\n",
        "        # \"Pies\",4\n",
        "        # \"Rest\"5\n",
        "\n",
        "desired_classes = [1, 2, 3, 4, 5]\n",
        "\n",
        "num_classes = len(desired_classes)\n",
        "\n",
        "# Filtramos los ensayos y etiquetas para mantener solo las clases deseadas\n",
        "filtered_indices = np.isin(dataConcatenada.labels, desired_classes)\n",
        "trials = dataConcatenada.trials[filtered_indices]\n",
        "labels = dataConcatenada.labels[filtered_indices]\n",
        "\n",
        "if 3 not in desired_classes:\n",
        "  if (5 in desired_classes) and (4 in desired_classes):\n",
        "    labels[labels == 4] = 3\n",
        "    labels[labels == 5] = 4\n",
        "  elif (5 in desired_classes):\n",
        "    labels[labels == 5] = 3\n",
        "\n",
        "print(trials.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "labels = labels-1\n",
        "\n",
        "eeg_train, eeg_test, labels_train, labels_test = train_test_split(trials, labels, test_size=0.2, stratify=labels)\n",
        "\n",
        "### ********** Instanciamos los diferentes objetos que usaremos en el pipeline**********\n",
        "\n",
        "fm = 250. #frecuencia de muestreo\n",
        "filtro = Filter(highcut = 30)\n",
        "csp = CSPMulticlass(n_components=2, method = \"ovo\", n_classes = num_classes, reg = 0.01)\n",
        "featureExtractor = FeatureExtractor(method = \"welch\", sample_rate = fm, axisToCompute=2, band_values=[8,12])\n",
        "ravelTransformer = RavelTransformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YYfQPA5yAD35",
      "metadata": {
        "id": "YYfQPA5yAD35"
      },
      "source": [
        "# CRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "bBymk-7vPTMt",
      "metadata": {
        "id": "bBymk-7vPTMt"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "class ChannelScaler(TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        n_channels = X.shape[1]\n",
        "        self.scalers = [StandardScaler() for _ in range(n_channels)]\n",
        "        for i in range(n_channels):\n",
        "            self.scalers[i].fit(X[:, i, :])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        n_channels = X.shape[1]\n",
        "        X_scaled = np.empty_like(X)\n",
        "        for i in range(n_channels):\n",
        "            X_scaled[:, i, :] = self.scalers[i].transform(X[:, i, :])\n",
        "        return X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "u9vcDTycKBeU",
      "metadata": {
        "id": "u9vcDTycKBeU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 6, 1000)\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.6470 - accuracy: 0.2095\n",
            "Epoch 1: val_loss improved from inf to 1.63480, saving model to best_model.h5\n",
            "4/4 [==============================] - 7s 865ms/step - loss: 1.6470 - accuracy: 0.2095 - val_loss: 1.6348 - val_accuracy: 0.1364\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - ETA: 0s - loss: 1.5384 - accuracy: 0.3143\n",
            "Epoch 2: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 625ms/step - loss: 1.5384 - accuracy: 0.3143 - val_loss: 1.6414 - val_accuracy: 0.0909\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.5340 - accuracy: 0.3619\n",
            "Epoch 3: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 622ms/step - loss: 1.5340 - accuracy: 0.3619 - val_loss: 1.6445 - val_accuracy: 0.0909\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.4868 - accuracy: 0.4476\n",
            "Epoch 4: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 612ms/step - loss: 1.4868 - accuracy: 0.4476 - val_loss: 1.6482 - val_accuracy: 0.0455\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.4853 - accuracy: 0.3714\n",
            "Epoch 5: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 615ms/step - loss: 1.4853 - accuracy: 0.3714 - val_loss: 1.6524 - val_accuracy: 0.0455\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.4232 - accuracy: 0.5333\n",
            "Epoch 6: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 617ms/step - loss: 1.4232 - accuracy: 0.5333 - val_loss: 1.6552 - val_accuracy: 0.0455\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.3786 - accuracy: 0.5619\n",
            "Epoch 7: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 622ms/step - loss: 1.3786 - accuracy: 0.5619 - val_loss: 1.6576 - val_accuracy: 0.0909\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.3724 - accuracy: 0.5333\n",
            "Epoch 8: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 625ms/step - loss: 1.3724 - accuracy: 0.5333 - val_loss: 1.6604 - val_accuracy: 0.0909\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.3398 - accuracy: 0.5619\n",
            "Epoch 9: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 620ms/step - loss: 1.3398 - accuracy: 0.5619 - val_loss: 1.6610 - val_accuracy: 0.2273\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.2870 - accuracy: 0.5810\n",
            "Epoch 10: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 627ms/step - loss: 1.2870 - accuracy: 0.5810 - val_loss: 1.6605 - val_accuracy: 0.1818\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.2577 - accuracy: 0.6000\n",
            "Epoch 11: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 629ms/step - loss: 1.2577 - accuracy: 0.6000 - val_loss: 1.6620 - val_accuracy: 0.1818\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.1643 - accuracy: 0.7333\n",
            "Epoch 12: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 617ms/step - loss: 1.1643 - accuracy: 0.7333 - val_loss: 1.6713 - val_accuracy: 0.1818\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.2086 - accuracy: 0.6190\n",
            "Epoch 13: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 611ms/step - loss: 1.2086 - accuracy: 0.6190 - val_loss: 1.6866 - val_accuracy: 0.1364\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.1443 - accuracy: 0.6762\n",
            "Epoch 14: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 621ms/step - loss: 1.1443 - accuracy: 0.6762 - val_loss: 1.7038 - val_accuracy: 0.1364\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.1046 - accuracy: 0.7048\n",
            "Epoch 15: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 624ms/step - loss: 1.1046 - accuracy: 0.7048 - val_loss: 1.7230 - val_accuracy: 0.1364\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0758 - accuracy: 0.6762Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.63480\n",
            "4/4 [==============================] - 3s 662ms/step - loss: 1.0758 - accuracy: 0.6762 - val_loss: 1.7430 - val_accuracy: 0.1364\n",
            "Epoch 16: early stopping\n"
          ]
        }
      ],
      "source": [
        "from CNNEEG import CRNN_EEGNet\n",
        "# Filtramos los ensayos y etiquetas para mantener solo las clases deseadas\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pasabanda', filtro),\n",
        "    ('standardnormalization', ChannelScaler())])\n",
        "\n",
        "# crnn_eegnet.fit(X_train, y_train, epochs=7000, validation_data=(X_val, y_val), patience=3000)\n",
        "pipeline.fit(trials, labels)\n",
        "\n",
        "transformed_trials = pipeline.transform(trials)\n",
        "print(transformed_trials.shape)\n",
        "\n",
        "labels_onehot = to_categorical(labels, num_classes = num_classes)\n",
        "\n",
        "# # Dividir en conjunto de entrenamiento y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(transformed_trials, labels_onehot, test_size=0.3, random_state=42, stratify= labels_onehot)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify= y_val)\n",
        "\n",
        "# X_train = np.expand_dims(X_train, axis=-1)\n",
        "# X_val = np.expand_dims(X_val, axis=-1)\n",
        "# X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "input_shape = X_train.shape[1:]  # Tomar la forma de los datos, excluyendo la dimensión de los ejemplos\n",
        "\n",
        "crnn_eegnet = CRNN_EEGNet(input_shape= input_shape, num_classes = num_classes)\n",
        "crnn_eegnet.compile()\n",
        "\n",
        "fitness = crnn_eegnet.fit(X_train, y_train, epochs = 100, validation_data = (X_val, y_val), patience = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "EGFOTfODC6ob",
      "metadata": {
        "id": "EGFOTfODC6ob"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.25      0.33         4\n",
            "           1       1.00      0.20      0.33         5\n",
            "           2       0.75      0.60      0.67         5\n",
            "           3       0.17      0.25      0.20         4\n",
            "           4       0.30      0.60      0.40         5\n",
            "\n",
            "    accuracy                           0.39        23\n",
            "   macro avg       0.54      0.38      0.39        23\n",
            "weighted avg       0.56      0.39      0.40        23\n",
            "\n",
            "\n",
            "[[1 0 0 0 3]\n",
            " [0 1 0 3 1]\n",
            " [0 0 3 1 1]\n",
            " [1 0 0 1 2]\n",
            " [0 0 1 1 3]]\n",
            "El accuracy del modelo CRNN_EEGNet es de 39.0\n",
            "El AUC promedio del modelo CRNN_EEGNet es de 0.70\n"
          ]
        }
      ],
      "source": [
        "# 1. Obtener las predicciones del modelo CRNN_EEGNet\n",
        "probs = crnn_eegnet.model.predict(X_test)  # Esto nos da las probabilidades\n",
        "\n",
        "y_pred = np.argmax(probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)  # Convertir y_test de formato one-hot a índices de clases\n",
        "\n",
        "print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
        "\n",
        "cm_crnn = crnn_eegnet\n",
        "# Matriz de confusión\n",
        "cm_crnn = confusion_matrix(y_true, y_pred)\n",
        "print(cm_crnn)\n",
        "\n",
        "# Métricas individuales\n",
        "precision_crnn, recall_crnn, f1score_crnn, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "# Accuracy\n",
        "acc_crnn = accuracy_score(y_true, y_pred)\n",
        "acc_crnn = np.round(acc_crnn, decimals=2) * 100\n",
        "print(f\"El accuracy del modelo CRNN_EEGNet es de {acc_crnn}\")\n",
        "\n",
        "# Calcular la curva ROC y el AUC\n",
        "n_classes = len(np.unique(y_true))\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true == i, probs[:, i])\n",
        "\n",
        "    if not np.any(np.isnan(tpr)): # Solo calcula AUC si no hay NaNs\n",
        "        roc_aucs.append(auc(fpr, tpr))\n",
        "\n",
        "auc_CRNN = np.mean(roc_aucs)\n",
        "print(f\"El AUC promedio del modelo CRNN_EEGNet es de {auc_CRNN:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Po77fr2eAS7D",
      "metadata": {
        "id": "Po77fr2eAS7D"
      },
      "source": [
        "# Creación de DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "RCS5iCaC1sm0",
      "metadata": {
        "id": "RCS5iCaC1sm0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Accuracy  Precision  Recall  F1-Score       AUC\n",
            "CRNN      39.0   0.543333    0.38  0.386667  0.703918\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\"])\n",
        "df.loc[\"CRNN\"] = [acc_crnn,precision_crnn, recall_crnn, f1score_crnn, auc_CRNN]  # We only have accuracy for CRNN in this case\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Save and return the best estimator for each pipeline\n",
        "best_estimators = {}\n",
        "best_estimators['CRNN'] = crnn_eegnet\n",
        "\n",
        "#Matrices de confusión\n",
        "\n",
        "cm = {'CRNN': cm_crnn}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "804b014b",
      "metadata": {
        "id": "804b014b"
      },
      "outputs": [],
      "source": [
        "# DataFrame with accuracy and other metrics\n",
        "results_df_path = \"Resultados/results_df.csv\"\n",
        "df.to_csv(results_df_path)\n",
        "\n",
        "# Save best estimators\n",
        "best_estimators_paths = {\n",
        "    'CRNN': \"Resultados/best_crnn.pkl\"\n",
        "}\n",
        "\n",
        "for model_name, path in best_estimators_paths.items():\n",
        "    with open(path, 'wb') as file:\n",
        "        pickle.dump(best_estimators[model_name], file)\n",
        "\n",
        "#Guardando matrices de confusión\n",
        "with open(\"Resultados/cm.pkl\", 'wb') as file:\n",
        "        pickle.dump(cm, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
