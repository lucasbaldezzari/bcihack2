{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YHBYXo0b-OH8",
      "metadata": {
        "id": "YHBYXo0b-OH8"
      },
      "source": [
        "# Instalación librería"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mU1qKwJk-TzM",
      "metadata": {
        "id": "mU1qKwJk-TzM"
      },
      "source": [
        "# Carga librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "skQH8zOtvX4t",
      "metadata": {
        "id": "skQH8zOtvX4t"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from SignalProcessor.Filter import Filter\n",
        "from SignalProcessor.CSPMulticlass import CSPMulticlass\n",
        "from SignalProcessor.FeatureExtractor import FeatureExtractor\n",
        "from SignalProcessor.RavelTransformer import RavelTransformer\n",
        "\n",
        "\n",
        "## Clasificadores LDA y SVM\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
        "import pickle\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from TrialsHandler.TrialsHandler import TrialsHandler\n",
        "from TrialsHandler.Concatenate import Concatenate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rnWVMtAI-Wnz",
      "metadata": {
        "id": "rnWVMtAI-Wnz"
      },
      "source": [
        "# Subida de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "hoob1OpAdekW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoob1OpAdekW",
        "outputId": "5d5740fc-7a53-4ea2-900c-e63b4b5c2321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se han extraido 75 trials\n",
            "Se han extraido 8 canales\n",
            "Se han extraido 1000 muestras por trial\n",
            "Se han extraido 75 trials\n",
            "Se han extraido 8 canales\n",
            "Se han extraido 1000 muestras por trial\n",
            "(150, 6, 1000)\n",
            "(150,)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "### ********** Cargamos los datos **********\n",
        "file = \"data/sujeto_4/eegdata/sesion1/sn1_ts0_ct0_r1.npy\"\n",
        "eventosFile = \"data/sujeto_4/eegdata/sesion1/sn1_ts0_ct1_r1_events.txt\"\n",
        "rawEEG_1 = np.load(file)\n",
        "eventos_1 = pd.read_csv(eventosFile, sep = \",\")\n",
        "\n",
        "file = \"data/sujeto_4/eegdata/sesion2/sn2_ts0_ct1_r1.npy\"\n",
        "eventosFile = \"data/sujeto_4/eegdata/sesion2/sn2_ts0_ct1_r1_events.txt\"\n",
        "rawEEG_2 = np.load(file)\n",
        "eventos_2 = pd.read_csv(eventosFile, sep = \",\")\n",
        "\n",
        "#Creamos objetos para manejar los trials\n",
        "th_1 = TrialsHandler(rawEEG_1, eventos_1, tinit = 0, tmax = 4, reject=None, sample_rate=250.)\n",
        "th_2 = TrialsHandler(rawEEG_2, eventos_2, tinit = 0, tmax = 4, reject=None, sample_rate=250.)\n",
        "\n",
        "dataConcatenada = Concatenate([th_1, th_2])#concatenamos datos\n",
        "\n",
        "channelsSelected = [0,1,2,3,6,7]\n",
        "\n",
        "#me quedo con channelsSelected\n",
        "dataConcatenada.trials = dataConcatenada.trials[:,channelsSelected,:]\n",
        "\n",
        "# Estas son las clases que quieres mantener\n",
        "        # \"Mano Izquierda\",1\n",
        "        # \"Mano Derecha\",2\n",
        "        # \"Ambas Manos\",3\n",
        "        # \"Pies\",4\n",
        "        # \"Rest\"5\n",
        "\n",
        "desired_classes = [1, 2, 3, 4, 5]\n",
        "\n",
        "num_classes = len(desired_classes)\n",
        "\n",
        "# Filtramos los ensayos y etiquetas para mantener solo las clases deseadas\n",
        "filtered_indices = np.isin(dataConcatenada.labels, desired_classes)\n",
        "trials = dataConcatenada.trials[filtered_indices]\n",
        "labels = dataConcatenada.labels[filtered_indices]\n",
        "\n",
        "if 3 not in desired_classes:\n",
        "  if (5 in desired_classes) and (4 in desired_classes):\n",
        "    labels[labels == 4] = 3\n",
        "    labels[labels == 5] = 4\n",
        "  elif (5 in desired_classes):\n",
        "    labels[labels == 5] = 3\n",
        "\n",
        "print(trials.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "labels = labels-1\n",
        "\n",
        "eeg_train, eeg_test, labels_train, labels_test = train_test_split(trials, labels, test_size=0.2, stratify=labels)\n",
        "\n",
        "### ********** Instanciamos los diferentes objetos que usaremos en el pipeline**********\n",
        "\n",
        "fm = 250. #frecuencia de muestreo\n",
        "filtro = Filter(highcut = 30)\n",
        "csp = CSPMulticlass(n_components=2, method = \"ovo\", n_classes = num_classes, reg = 0.01)\n",
        "featureExtractor = FeatureExtractor(method = \"welch\", sample_rate = fm, axisToCompute=2, band_values=[8,12])\n",
        "ravelTransformer = RavelTransformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uvjyRnTP-dBa",
      "metadata": {
        "id": "uvjyRnTP-dBa"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8Nrf3zVPTKI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Nrf3zVPTKI",
        "outputId": "623aafad-c574-4fcb-863f-2551e98af1d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 416, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 370, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 950, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py\", line 918, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\SignalProcessor\\CSPMulticlass.py\", line 213, in transform\n    X_transformed = [csp.transform(X) for csp in self.csplist]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\SignalProcessor\\CSPMulticlass.py\", line 213, in <listcomp>\n    X_transformed = [csp.transform(X) for csp in self.csplist]\n                     ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\mne\\decoding\\csp.py\", line 224, in transform\n    if self.filters_ is None:\n       ^^^^^^^^^^^^^\nAttributeError: 'CSP' object has no attribute 'filters_'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\BCIColabb.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m random_search_lda \u001b[39m=\u001b[39m RandomizedSearchCV(pipeline_lda, param_distributions\u001b[39m=\u001b[39mparam_distributions_lda, n_iter\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m### ********** Entrenamos el modelo **********\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m random_search_lda\u001b[39m.\u001b[39;49mfit(eeg_train, labels_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m### ******************************************\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m### Nos quedamos con el mejor estimador\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/Repos/bcihack2/Desarrollo/PythonScripts/scripts/BCIColabb.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m best_lda \u001b[39m=\u001b[39m random_search_lda\u001b[39m.\u001b[39mbest_estimator_\n",
            "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1807\u001b[0m         ParameterSampler(\n\u001b[0;32m   1808\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1809\u001b[0m         )\n\u001b[0;32m   1810\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    877\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
            "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 416, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 370, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 950, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py\", line 918, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\SignalProcessor\\CSPMulticlass.py\", line 213, in transform\n    X_transformed = [csp.transform(X) for csp in self.csplist]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\SignalProcessor\\CSPMulticlass.py\", line 213, in <listcomp>\n    X_transformed = [csp.transform(X) for csp in self.csplist]\n                     ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\mne\\decoding\\csp.py\", line 224, in transform\n    if self.filters_ is None:\n       ^^^^^^^^^^^^^\nAttributeError: 'CSP' object has no attribute 'filters_'\n"
          ]
        }
      ],
      "source": [
        "# Instanciamos un LDA\n",
        "lda = LinearDiscriminantAnalysis()  # instanciamos el clasificador LDA\n",
        "\n",
        "### ********** Creamos el pipeline para LDA **********\n",
        "\n",
        "pipeline_lda = Pipeline([\n",
        "    ('pasabanda', filtro),\n",
        "    ('cspmulticlase', csp),\n",
        "    ('featureExtractor', featureExtractor),\n",
        "    ('ravelTransformer', ravelTransformer),\n",
        "    ('standardScaler', StandardScaler()),\n",
        "    ('lda', lda)\n",
        "])\n",
        "\n",
        "### ********** Creamos la grilla de hiperparámetros **********\n",
        "\n",
        "param_distributions_lda = {\n",
        "    'pasabanda__lowcut': [5],\n",
        "    'pasabanda__highcut': [16, 30],\n",
        "    'cspmulticlase__n_components': [2, 3],\n",
        "    'cspmulticlase__method': [\"ovo\",\"ova\"],\n",
        "    'cspmulticlase__reg': [0.01],\n",
        "    'cspmulticlase__log': [None],\n",
        "    'cspmulticlase__norm_trace': [False],\n",
        "    'featureExtractor__method': [\"welch\", \"hilbert\"],\n",
        "    'featureExtractor__sample_rate': [fm],\n",
        "    'featureExtractor__band_values': [[8, 12]],\n",
        "    'lda__solver': ['svd', 'lsqr'],\n",
        "    'lda__tol': [0.01, 0.001]\n",
        "}\n",
        "\n",
        "# Creamos el RandomizedSearch para el LDA\n",
        "# Aquí puedes ajustar n_iter según cuántas iteraciones de búsqueda aleatoria quieras realizar\n",
        "random_search_lda = RandomizedSearchCV(pipeline_lda, param_distributions=param_distributions_lda, n_iter=2, cv=5, n_jobs=3, verbose=2, random_state=42)\n",
        "\n",
        "### ********** Entrenamos el modelo **********\n",
        "random_search_lda.fit(eeg_train, labels_train)\n",
        "### ******************************************\n",
        "\n",
        "### Nos quedamos con el mejor estimador\n",
        "best_lda = random_search_lda.best_estimator_\n",
        "\n",
        "# Reporte de clasificación\n",
        "y_true, y_pred = labels_test, best_lda.predict(eeg_test)\n",
        "print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
        "\n",
        "## Creamos una matriz de confusión\n",
        "cm_lda = confusion_matrix(y_true, y_pred)\n",
        "print(cm_lda)\n",
        "\n",
        "## obtenemos precision, recall y f1-score y los guardamos en variables\n",
        "precision_lda, recall_lda, f1score_lda, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "## Obtenemos el accuracy y lo redondeamos a 2 decimales\n",
        "acc_lda = accuracy_score(y_true, y_pred)\n",
        "acc_lda = np.round(acc_lda, decimals=2) * 100\n",
        "print(f\"El accuracy del mejor clasificador LDA es de {acc_lda}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WaIZMUwLYGiR",
      "metadata": {
        "id": "WaIZMUwLYGiR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.17      0.15         6\n",
            "           1       0.20      0.17      0.18         6\n",
            "           2       0.50      0.50      0.50         6\n",
            "\n",
            "    accuracy                           0.28        18\n",
            "   macro avg       0.28      0.28      0.28        18\n",
            "weighted avg       0.28      0.28      0.28        18\n",
            "\n",
            "\n",
            "El AUC promedio para todas las clases es: 0.43\n"
          ]
        }
      ],
      "source": [
        "if num_classes == 2:\n",
        "  from sklearn.metrics import roc_curve, auc, classification_report\n",
        "\n",
        "  # Reporte de clasificación\n",
        "  y_true, y_pred = labels_test, best_lda.predict(eeg_test)\n",
        "  print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
        "\n",
        "  # Obtener la función de decisión o probabilidad para las clases\n",
        "  try:\n",
        "      y_score = best_lda.decision_function(eeg_test)\n",
        "  except AttributeError:\n",
        "      y_prob = best_lda.predict_proba(eeg_test)\n",
        "      y_score = y_prob[:, 1]  # Solo nos interesa la columna de la clase positiva\n",
        "\n",
        "  # Calcula la curva ROC y el AUC para la clase positiva\n",
        "  fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=2)\n",
        "  auc_LDA = auc(fpr, tpr)\n",
        "\n",
        "  print(f\"El AUC para la clase positiva es: {auc_LDA:.2f}\")\n",
        "\n",
        "else:\n",
        "\n",
        "  # Binariza las etiquetas\n",
        "  y_true_bin = label_binarize(labels_test, classes=np.unique(labels_test))\n",
        "\n",
        "  # Asegúrate de que y_score es una matriz con una columna por clase\n",
        "  # Reporte de clasificación\n",
        "  y_true, y_pred = labels_test, best_lda.predict(eeg_test)\n",
        "  print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
        "\n",
        "  # Obtener la función de decisión o probabilidad para las clases\n",
        "  try:\n",
        "      y_score = best_lda.decision_function(eeg_test)\n",
        "  except AttributeError:\n",
        "      y_prob = best_lda.predict_proba(eeg_test)\n",
        "      y_score = y_prob\n",
        "\n",
        "  # Inicializa contenedores para FPR y TPR\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "\n",
        "  # Calcula la curva ROC y el AUC para cada clase\n",
        "  for i in range(y_true_bin.shape[1]):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  # Si deseas, puedes calcular una curva ROC y un AUC promedio para todas las clases\n",
        "  fpr[\"macro\"], tpr[\"macro\"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
        "  roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "  auc_LDA = roc_auc['macro']\n",
        "\n",
        "  # Ahora puedes plotear las curvas ROC para cada clase o el promedio.\n",
        "  print(f\"El AUC promedio para todas las clases es: {auc_LDA:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hgZSrPTh-uo9",
      "metadata": {
        "id": "hgZSrPTh-uo9"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47IH7wknUUlR",
      "metadata": {
        "id": "47IH7wknUUlR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.7e+15  max singular value)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
            "15 fits failed out of a total of 25.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 416, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 370, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\pipeline.py\", line 950, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\base.py\", line 918, in fit_transform\n",
            "    return self.fit(X, y, **fit_params).transform(X)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 140, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\SignalProcessor\\CSPMulticlass.py\", line 213, in transform\n",
            "    X_transformed = [csp.transform(X) for csp in self.csplist]\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\Documents\\Repos\\bcihack2\\Desarrollo\\PythonScripts\\scripts\\SignalProcessor\\CSPMulticlass.py\", line 213, in <listcomp>\n",
            "    X_transformed = [csp.transform(X) for csp in self.csplist]\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\mne\\decoding\\csp.py\", line 224, in transform\n",
            "    if self.filters_ is None:\n",
            "       ^^^^^^^^^^^^^\n",
            "AttributeError: 'CSP' object has no attribute 'filters_'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.29166667        nan 0.3               nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.1e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.1e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.4e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.1e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.3e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.1e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.7e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.4e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.7e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.3e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.7e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.4e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.3e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.3e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 12 (2.2e-16 eps * 6 dim * 9.4e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.67      0.53         6\n",
            "           1       0.29      0.33      0.31         6\n",
            "           2       0.17      0.17      0.17         6\n",
            "           3       0.00      0.00      0.00         6\n",
            "           4       0.75      0.50      0.60         6\n",
            "\n",
            "    accuracy                           0.33        30\n",
            "   macro avg       0.33      0.33      0.32        30\n",
            "weighted avg       0.33      0.33      0.32        30\n",
            "\n",
            "\n",
            "[[4 1 0 0 1]\n",
            " [1 2 1 2 0]\n",
            " [3 2 1 0 0]\n",
            " [1 2 3 0 0]\n",
            " [0 0 1 2 3]]\n",
            "El accuracy del mejor clasificador SVM es de 33.0\n"
          ]
        }
      ],
      "source": [
        "# Instanciamos un SVM\n",
        "svm = SVC(probability=True)  # instanciamos el clasificador SVM con la opción de probabilidad habilitada\n",
        "\n",
        "### ********** Creamos el pipeline para SVM **********\n",
        "\n",
        "pipeline_svm = Pipeline([\n",
        "    ('pasabanda', filtro),\n",
        "    ('cspmulticlase', csp),\n",
        "    ('featureExtractor', featureExtractor),\n",
        "    ('ravelTransformer', ravelTransformer),\n",
        "    ('standardScaler', StandardScaler()),\n",
        "    ('svm', svm)\n",
        "])\n",
        "\n",
        "### ********** Creamos la grilla de hiperparámetros **********\n",
        "\n",
        "grid_svm = {\n",
        "    'pasabanda__lowcut': [5],\n",
        "    'pasabanda__highcut': [16, 30],\n",
        "    'cspmulticlase__n_components': [2, 3],\n",
        "    'cspmulticlase__method': [\"ovo\",\"ova\"],\n",
        "    'cspmulticlase__reg': [0.01],\n",
        "    'cspmulticlase__log': [None],\n",
        "    'cspmulticlase__norm_trace': [False],\n",
        "    'featureExtractor__method': [\"welch\", \"hilbert\"],\n",
        "    'featureExtractor__sample_rate': [fm],\n",
        "    'featureExtractor__band_values': [[8, 12]],\n",
        "    'svm__C': [0.1, 1, 10],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Tipo de kernel\n",
        "    'svm__gamma': ['scale', 'auto']  # Coeficiente de kernel para 'rbf'\n",
        "}\n",
        "\n",
        "# Creamos el RandomizedSearch para el SVM\n",
        "# Aquí puedes ajustar n_iter según cuántas iteraciones de búsqueda aleatoria quieras realizar\n",
        "random_search_svm = RandomizedSearchCV(pipeline_svm, param_distributions=grid_svm, n_iter=5, cv=5, n_jobs=3, verbose=0, random_state=42)\n",
        "\n",
        "### ********** Entrenamos el modelo **********\n",
        "random_search_svm.fit(eeg_train, labels_train)\n",
        "\n",
        "### ******************************************\n",
        "\n",
        "### Nos quedamos con el mejor estimador\n",
        "best_svm = random_search_svm.best_estimator_\n",
        "\n",
        "# Reporte de clasificación\n",
        "y_true, y_pred = labels_test, best_svm.predict(eeg_test)\n",
        "print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
        "\n",
        "## Creamos una matriz de confusión\n",
        "cm_svm = confusion_matrix(y_true, y_pred)\n",
        "print(cm_svm)\n",
        "\n",
        "## obtenemos precision, recall y f1-score y los guardamos en variables\n",
        "precision_svm, recall_svm, f1score_svm, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "## Obtenemos el accuracy y lo redondeamos a 2 decimales\n",
        "acc_svm = accuracy_score(y_true, y_pred)\n",
        "acc_svm = np.round(acc_svm, decimals=2)*100\n",
        "print(f\"El accuracy del mejor clasificador SVM es de {acc_svm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQmqf0boR9xW",
      "metadata": {
        "id": "mQmqf0boR9xW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El AUC promedio del clasificador SVM es de 0.47\n"
          ]
        }
      ],
      "source": [
        "if num_classes == 2:\n",
        "  # y_prob = best_svm.predict_proba(eeg_test)\n",
        "\n",
        "  # # Usa solo la columna correspondiente a la clase positiva para el score\n",
        "  # y_score_pos_class = y_prob[:, 1]\n",
        "\n",
        "  # # Calcula la curva ROC y el AUC para la clase positiva\n",
        "  # fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score_pos_class.ravel())\n",
        "  # auc_SVM = auc(fpr, tpr)\n",
        "\n",
        "  # print(f\"El AUC para la clase positiva es: {auc_SVM:.2f}\")\n",
        "  y_prob = best_svm.predict_proba(eeg_test)\n",
        "\n",
        "  # Define las etiquetas verdaderas en formato binario\n",
        "  y_true_bin = np.where(y_true == 1, 1, 0)  # Asume que 1 es la clase positiva\n",
        "\n",
        "  # Usa solo la columna correspondiente a la clase positiva para el score\n",
        "  y_score_pos_class = y_prob[:, 1]\n",
        "\n",
        "  # Calcula la curva ROC y el AUC para la clase positiva\n",
        "  fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score_pos_class.ravel())\n",
        "  auc_SVM = auc(fpr, tpr)\n",
        "\n",
        "  print(f\"El AUC para la clase positiva es: {auc_SVM:.2f}\")\n",
        "\n",
        "else:\n",
        "  y_prob = best_svm.predict_proba(eeg_test)\n",
        "\n",
        "  # Calcular la curva ROC y el AUC\n",
        "  n_classes = len(np.unique(labels_train))\n",
        "  roc_aucs = []\n",
        "\n",
        "  for i in range(n_classes):\n",
        "      fpr, tpr, _ = roc_curve(labels_test == i, y_prob[:, i])\n",
        "\n",
        "      if not np.any(np.isnan(tpr)):  # Solo calcula AUC si no hay NaNs\n",
        "          roc_aucs.append(auc(fpr, tpr))\n",
        "\n",
        "  auc_SVM = np.mean(roc_aucs)\n",
        "  print(f\"El AUC promedio del clasificador SVM es de {auc_SVM:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YYfQPA5yAD35",
      "metadata": {
        "id": "YYfQPA5yAD35"
      },
      "source": [
        "# CRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bBymk-7vPTMt",
      "metadata": {
        "id": "bBymk-7vPTMt"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "class ChannelScaler(TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.scalers = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        n_channels = X.shape[1]\n",
        "        self.scalers = [StandardScaler() for _ in range(n_channels)]\n",
        "        for i in range(n_channels):\n",
        "            self.scalers[i].fit(X[:, i, :])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        n_channels = X.shape[1]\n",
        "        X_scaled = np.empty_like(X)\n",
        "        for i in range(n_channels):\n",
        "            X_scaled[:, i, :] = self.scalers[i].transform(X[:, i, :])\n",
        "        return X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u9vcDTycKBeU",
      "metadata": {
        "id": "u9vcDTycKBeU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(90, 6, 1000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.1367 - accuracy: 0.3175\n",
            "Epoch 1: val_loss improved from inf to 1.10563, saving model to best_model.h5\n",
            "2/2 [==============================] - 6s 1s/step - loss: 1.1367 - accuracy: 0.3175 - val_loss: 1.1056 - val_accuracy: 0.3846\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\miniconda3\\envs\\bcihack-GUI\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - ETA: 0s - loss: 1.0929 - accuracy: 0.3968\n",
            "Epoch 2: val_loss improved from 1.10563 to 1.10199, saving model to best_model.h5\n",
            "2/2 [==============================] - 2s 911ms/step - loss: 1.0929 - accuracy: 0.3968 - val_loss: 1.1020 - val_accuracy: 0.3846\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.0517 - accuracy: 0.5238\n",
            "Epoch 3: val_loss improved from 1.10199 to 1.10101, saving model to best_model.h5\n",
            "2/2 [==============================] - 2s 1s/step - loss: 1.0517 - accuracy: 0.5238 - val_loss: 1.1010 - val_accuracy: 0.3077\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9622 - accuracy: 0.6667\n",
            "Epoch 4: val_loss improved from 1.10101 to 1.09975, saving model to best_model.h5\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.9622 - accuracy: 0.6667 - val_loss: 1.0998 - val_accuracy: 0.3077\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9714 - accuracy: 0.6508\n",
            "Epoch 5: val_loss improved from 1.09975 to 1.09908, saving model to best_model.h5\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.9714 - accuracy: 0.6508 - val_loss: 1.0991 - val_accuracy: 0.3846\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9164 - accuracy: 0.7778\n",
            "Epoch 6: val_loss improved from 1.09908 to 1.09888, saving model to best_model.h5\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.9164 - accuracy: 0.7778 - val_loss: 1.0989 - val_accuracy: 0.3846\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.8947 - accuracy: 0.7460\n",
            "Epoch 7: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.8947 - accuracy: 0.7460 - val_loss: 1.0989 - val_accuracy: 0.4615\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.8846 - accuracy: 0.7937\n",
            "Epoch 8: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 835ms/step - loss: 0.8846 - accuracy: 0.7937 - val_loss: 1.1000 - val_accuracy: 0.4615\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.8594 - accuracy: 0.7937\n",
            "Epoch 9: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 831ms/step - loss: 0.8594 - accuracy: 0.7937 - val_loss: 1.1032 - val_accuracy: 0.4615\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.8236 - accuracy: 0.8571\n",
            "Epoch 10: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 863ms/step - loss: 0.8236 - accuracy: 0.8571 - val_loss: 1.1072 - val_accuracy: 0.4615\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7866 - accuracy: 0.8254\n",
            "Epoch 11: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.7866 - accuracy: 0.8254 - val_loss: 1.1114 - val_accuracy: 0.4615\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7282 - accuracy: 0.9048\n",
            "Epoch 12: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.7282 - accuracy: 0.9048 - val_loss: 1.1159 - val_accuracy: 0.3846\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6989 - accuracy: 0.8413\n",
            "Epoch 13: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.6989 - accuracy: 0.8413 - val_loss: 1.1222 - val_accuracy: 0.3846\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.8889\n",
            "Epoch 14: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.6698 - accuracy: 0.8889 - val_loss: 1.1285 - val_accuracy: 0.2308\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.9048\n",
            "Epoch 15: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 3s 1s/step - loss: 0.6561 - accuracy: 0.9048 - val_loss: 1.1351 - val_accuracy: 0.2308\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5678 - accuracy: 0.9365\n",
            "Epoch 16: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 3s 1s/step - loss: 0.5678 - accuracy: 0.9365 - val_loss: 1.1428 - val_accuracy: 0.1538\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.8889\n",
            "Epoch 17: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.5927 - accuracy: 0.8889 - val_loss: 1.1508 - val_accuracy: 0.1538\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.9206\n",
            "Epoch 18: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 3s 1s/step - loss: 0.5352 - accuracy: 0.9206 - val_loss: 1.1608 - val_accuracy: 0.1538\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.9365\n",
            "Epoch 19: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 993ms/step - loss: 0.4931 - accuracy: 0.9365 - val_loss: 1.1718 - val_accuracy: 0.1538\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.9524\n",
            "Epoch 20: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 2s/step - loss: 0.4400 - accuracy: 0.9524 - val_loss: 1.1822 - val_accuracy: 0.1538\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.9524Restoring model weights from the end of the best epoch: 6.\n",
            "\n",
            "Epoch 21: val_loss did not improve from 1.09888\n",
            "2/2 [==============================] - 2s 928ms/step - loss: 0.3946 - accuracy: 0.9524 - val_loss: 1.1937 - val_accuracy: 0.1538\n",
            "Epoch 21: early stopping\n"
          ]
        }
      ],
      "source": [
        "from CNNEEG import CRNN_EEGNet\n",
        "# Filtramos los ensayos y etiquetas para mantener solo las clases deseadas\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pasabanda', filtro),\n",
        "    ('standardnormalization', ChannelScaler())])\n",
        "\n",
        "# crnn_eegnet.fit(X_train, y_train, epochs=7000, validation_data=(X_val, y_val), patience=3000)\n",
        "pipeline.fit(trials, labels)\n",
        "\n",
        "transformed_trials = pipeline.transform(trials)\n",
        "print(transformed_trials.shape)\n",
        "\n",
        "labels_onehot = to_categorical(labels, num_classes = num_classes)\n",
        "\n",
        "# # Dividir en conjunto de entrenamiento y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(transformed_trials, labels_onehot, test_size=0.3, random_state=42, stratify= labels_onehot)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify= y_val)\n",
        "\n",
        "# X_train = np.expand_dims(X_train, axis=-1)\n",
        "# X_val = np.expand_dims(X_val, axis=-1)\n",
        "# X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "input_shape = X_train.shape[1:]  # Tomar la forma de los datos, excluyendo la dimensión de los ejemplos\n",
        "\n",
        "crnn_eegnet = CRNN_EEGNet(input_shape= input_shape, num_classes = num_classes)\n",
        "crnn_eegnet.compile()\n",
        "\n",
        "fitness = crnn_eegnet.fit(X_train, y_train, epochs = 100, validation_data = (X_val, y_val), patience = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EGFOTfODC6ob",
      "metadata": {
        "id": "EGFOTfODC6ob"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.60      0.55         5\n",
            "           1       0.67      0.40      0.50         5\n",
            "           2       0.40      0.50      0.44         4\n",
            "\n",
            "    accuracy                           0.50        14\n",
            "   macro avg       0.52      0.50      0.50        14\n",
            "weighted avg       0.53      0.50      0.50        14\n",
            "\n",
            "\n",
            "[[3 0 2]\n",
            " [2 2 1]\n",
            " [1 1 2]]\n",
            "El accuracy del modelo CRNN_EEGNet es de 50.0\n",
            "El AUC promedio del modelo CRNN_EEGNet es de 0.64\n"
          ]
        }
      ],
      "source": [
        "# 1. Obtener las predicciones del modelo CRNN_EEGNet\n",
        "probs = crnn_eegnet.model.predict(X_test)  # Esto nos da las probabilidades\n",
        "\n",
        "y_pred = np.argmax(probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)  # Convertir y_test de formato one-hot a índices de clases\n",
        "\n",
        "print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
        "\n",
        "cm_crnn = crnn_eegnet\n",
        "# Matriz de confusión\n",
        "cm_crnn = confusion_matrix(y_true, y_pred)\n",
        "print(cm_crnn)\n",
        "\n",
        "# Métricas individuales\n",
        "precision_crnn, recall_crnn, f1score_crnn, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "# Accuracy\n",
        "acc_crnn = accuracy_score(y_true, y_pred)\n",
        "acc_crnn = np.round(acc_crnn, decimals=2) * 100\n",
        "print(f\"El accuracy del modelo CRNN_EEGNet es de {acc_crnn}\")\n",
        "\n",
        "# Calcular la curva ROC y el AUC\n",
        "n_classes = len(np.unique(y_true))\n",
        "roc_aucs = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true == i, probs[:, i])\n",
        "\n",
        "    if not np.any(np.isnan(tpr)): # Solo calcula AUC si no hay NaNs\n",
        "        roc_aucs.append(auc(fpr, tpr))\n",
        "\n",
        "auc_CRNN = np.mean(roc_aucs)\n",
        "print(f\"El AUC promedio del modelo CRNN_EEGNet es de {auc_CRNN:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SCodS_EX2oBf",
      "metadata": {
        "id": "SCodS_EX2oBf"
      },
      "source": [
        "# Entrenar clasificadores con todo el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d9Teq62wBx",
      "metadata": {
        "id": "b5d9Teq62wBx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.9e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 20 (2.2e-16 eps * 6 dim * 1.5e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Using tolerance 19 (2.2e-16 eps * 6 dim * 1.4e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.8e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 20 (2.2e-16 eps * 6 dim * 1.5e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pasabanda&#x27;, Filter(highcut=16, lowcut=5)),\n",
              "                (&#x27;cspmulticlase&#x27;,\n",
              "                 CSPMulticlass(method=&#x27;ova&#x27;, n_classes=3, reg=0.01)),\n",
              "                (&#x27;featureExtractor&#x27;,\n",
              "                 FeatureExtractor(band_values=[8, 12], method=&#x27;hilbert&#x27;)),\n",
              "                (&#x27;ravelTransformer&#x27;, RavelTransformer()),\n",
              "                (&#x27;standardScaler&#x27;, StandardScaler()),\n",
              "                (&#x27;lda&#x27;, LinearDiscriminantAnalysis(solver=&#x27;lsqr&#x27;, tol=0.01))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pasabanda&#x27;, Filter(highcut=16, lowcut=5)),\n",
              "                (&#x27;cspmulticlase&#x27;,\n",
              "                 CSPMulticlass(method=&#x27;ova&#x27;, n_classes=3, reg=0.01)),\n",
              "                (&#x27;featureExtractor&#x27;,\n",
              "                 FeatureExtractor(band_values=[8, 12], method=&#x27;hilbert&#x27;)),\n",
              "                (&#x27;ravelTransformer&#x27;, RavelTransformer()),\n",
              "                (&#x27;standardScaler&#x27;, StandardScaler()),\n",
              "                (&#x27;lda&#x27;, LinearDiscriminantAnalysis(solver=&#x27;lsqr&#x27;, tol=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Filter</label><div class=\"sk-toggleable__content\"><pre>Filter(highcut=16, lowcut=5)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CSPMulticlass</label><div class=\"sk-toggleable__content\"><pre>CSPMulticlass(method=&#x27;ova&#x27;, n_classes=3, reg=0.01)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureExtractor</label><div class=\"sk-toggleable__content\"><pre>FeatureExtractor(band_values=[8, 12], method=&#x27;hilbert&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RavelTransformer</label><div class=\"sk-toggleable__content\"><pre>RavelTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis(solver=&#x27;lsqr&#x27;, tol=0.01)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('pasabanda', Filter(highcut=16, lowcut=5)),\n",
              "                ('cspmulticlase',\n",
              "                 CSPMulticlass(method='ova', n_classes=3, reg=0.01)),\n",
              "                ('featureExtractor',\n",
              "                 FeatureExtractor(band_values=[8, 12], method='hilbert')),\n",
              "                ('ravelTransformer', RavelTransformer()),\n",
              "                ('standardScaler', StandardScaler()),\n",
              "                ('lda', LinearDiscriminantAnalysis(solver='lsqr', tol=0.01))])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_lda.fit(trials, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qX7V_Ca626bm",
      "metadata": {
        "id": "qX7V_Ca626bm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.9e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.8e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.9e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 13 (2.2e-16 eps * 6 dim * 9.8e+15  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n",
            "Computing rank from data with rank=None\n",
            "    Using tolerance 15 (2.2e-16 eps * 6 dim * 1.1e+16  max singular value)\n",
            "    Estimated rank (mag): 6\n",
            "    MAG: rank 6 computed from 6 data channels with 0 projectors\n",
            "Reducing data rank from 6 -> 6\n",
            "Estimating covariance using SHRINKAGE\n",
            "Done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pasabanda&#x27;, Filter(highcut=16, lowcut=5)),\n",
              "                (&#x27;cspmulticlase&#x27;,\n",
              "                 CSPMulticlass(n_classes=3, n_components=3, reg=0.01)),\n",
              "                (&#x27;featureExtractor&#x27;,\n",
              "                 FeatureExtractor(band_values=[8, 12], method=&#x27;hilbert&#x27;)),\n",
              "                (&#x27;ravelTransformer&#x27;, RavelTransformer()),\n",
              "                (&#x27;standardScaler&#x27;, StandardScaler()),\n",
              "                (&#x27;svm&#x27;, SVC(C=1, kernel=&#x27;linear&#x27;, probability=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pasabanda&#x27;, Filter(highcut=16, lowcut=5)),\n",
              "                (&#x27;cspmulticlase&#x27;,\n",
              "                 CSPMulticlass(n_classes=3, n_components=3, reg=0.01)),\n",
              "                (&#x27;featureExtractor&#x27;,\n",
              "                 FeatureExtractor(band_values=[8, 12], method=&#x27;hilbert&#x27;)),\n",
              "                (&#x27;ravelTransformer&#x27;, RavelTransformer()),\n",
              "                (&#x27;standardScaler&#x27;, StandardScaler()),\n",
              "                (&#x27;svm&#x27;, SVC(C=1, kernel=&#x27;linear&#x27;, probability=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Filter</label><div class=\"sk-toggleable__content\"><pre>Filter(highcut=16, lowcut=5)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CSPMulticlass</label><div class=\"sk-toggleable__content\"><pre>CSPMulticlass(n_classes=3, n_components=3, reg=0.01)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureExtractor</label><div class=\"sk-toggleable__content\"><pre>FeatureExtractor(band_values=[8, 12], method=&#x27;hilbert&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RavelTransformer</label><div class=\"sk-toggleable__content\"><pre>RavelTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('pasabanda', Filter(highcut=16, lowcut=5)),\n",
              "                ('cspmulticlase',\n",
              "                 CSPMulticlass(n_classes=3, n_components=3, reg=0.01)),\n",
              "                ('featureExtractor',\n",
              "                 FeatureExtractor(band_values=[8, 12], method='hilbert')),\n",
              "                ('ravelTransformer', RavelTransformer()),\n",
              "                ('standardScaler', StandardScaler()),\n",
              "                ('svm', SVC(C=1, kernel='linear', probability=True))])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_svm.fit(trials, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Po77fr2eAS7D",
      "metadata": {
        "id": "Po77fr2eAS7D"
      },
      "source": [
        "# Creación de DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RCS5iCaC1sm0",
      "metadata": {
        "id": "RCS5iCaC1sm0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Accuracy  Precision    Recall  F1-Score       AUC\n",
            "LDA       28.0   0.280952  0.277778  0.278555  0.429012\n",
            "SVM       39.0   0.433333  0.388889  0.391667  0.467593\n",
            "CRNN      50.0   0.522222  0.500000  0.496633  0.643519\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\"])\n",
        "df.loc[\"LDA\"] = [acc_lda, precision_lda, recall_lda, f1score_lda, auc_LDA]\n",
        "df.loc[\"SVM\"] = [acc_svm, precision_svm, recall_svm, f1score_svm, auc_SVM]\n",
        "df.loc[\"CRNN\"] = [acc_crnn,precision_crnn, recall_crnn, f1score_crnn, auc_CRNN]  # We only have accuracy for CRNN in this case\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Save and return the best estimator for each pipeline\n",
        "best_estimators = {}\n",
        "best_estimators['LDA'] = best_lda\n",
        "best_estimators['SVM'] = best_svm\n",
        "best_estimators['CRNN'] = crnn_eegnet\n",
        "\n",
        "# Save and return a DataFrame with the best hyperparameters for each pipeline\n",
        "hyperparameters_data = {'LDA': [random_search_lda.best_params_], 'SVM': [random_search_svm.best_params_]}\n",
        "\n",
        "hyperparameters_df = pd.DataFrame(hyperparameters_data, index=['Best Parameters'])\n",
        "\n",
        "#Matrices de confusión\n",
        "\n",
        "cm = {'LDA': cm_lda,\n",
        "      'SVM': cm_svm,\n",
        "      'CRNN': cm_crnn}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "804b014b",
      "metadata": {
        "id": "804b014b"
      },
      "outputs": [],
      "source": [
        "# DataFrame with accuracy and other metrics\n",
        "results_df_path = \"Resultados/results_df.csv\"\n",
        "df.to_csv(results_df_path)\n",
        "\n",
        "# DataFrame with best hyperparameters\n",
        "hyperparameters_df_path = \"Resultados/hyperparameters_df.csv\"\n",
        "hyperparameters_df.to_csv(hyperparameters_df_path)\n",
        "\n",
        "# Save best estimators\n",
        "best_estimators_paths = {\n",
        "    'LDA': \"Resultados/best_lda.pkl\",\n",
        "    'SVM': \"Resultados/best_svm.pkl\",\n",
        "    'CRNN': \"Resultados/best_crnn.pkl\"\n",
        "}\n",
        "\n",
        "for model_name, path in best_estimators_paths.items():\n",
        "    with open(path, 'wb') as file:\n",
        "        pickle.dump(best_estimators[model_name], file)\n",
        "\n",
        "#Guardando matrices de confusión\n",
        "with open(\"Resultados/cm.pkl\", 'wb') as file:\n",
        "        pickle.dump(cm, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
